  

## 特别注意

1.   ==在哪个机器上==

2.   ==在哪个文件夹==

3.   ==用哪个用户==

4.   ```bash
     # 对的用户才能用, 才能看, 才能启动
     
     ```

5.   

6.   使用Sync同步时

     1.   尽量少同步文件, 覆盖后很麻烦
     2.   同步后, 看一眼有没有成功, 步步为营的好习惯

### yum  文件格式

```bash
# 每行必须顶格，不能有空格
# “：”后面必须有一个空格
```

### 排错

```bash
# 启动不起来, 删除data和log. 生产环境, 用 ==rm== 特别小心
# 
```

### 好习惯

```bash
# 关于配置
    1 改配置前先备份
    2 改完配置要分发
    3 分发完后要看一眼, 个别需要改参数
    4 改完参数再重启
```





## vim常用命令备忘

```bash
# 删除到行尾
dG
# 删除一行
dd
# 在下一行插入
o
# 复制n行
yny
ny
# 撤销
u
# 重做
ctrl+r
# 替换1个字母
r
# 显示行号
: set nu
# 取消行号
: set nonu
# 到第3行
3G
# 到行尾
$
# 到行首
^

```

## bash常用命令备忘

```bash
# 递归复制
cp -r 

# 递归创建目录
mkdir -p grand/parent/child

# 打包
tar -czvf dest.tar.gz file.txt file2.txt

# 解压
tar -xzvf sour.tar.gz -C /opt/module

# 查找
/ N

# 到行首
ctrl+a

# 到行尾
ctrl+e

# 直接加入用户组
usermod -aG wheel atguigu
# 更改sudoer后, 不需要source, 执行sudo命令时, 自动会检查sudoers文件


```

## 查看状态

```bash
# 通过进程名字看状态
$ ps -ef | grep

# 通过端口号看状态
$ sudo netstat -nltp | grep

# 看看网址返回什么
curl http://hadoop102:9200/_cat/nodes?v


```



## tar

```bash
# tar压缩多个文件：
tar -czvf a.tar.gz hello.txt world.txt
# tar压缩目录：
tar -czvf hello.tar.gz opt/
# tar解压到当前目录：
tar -xzvf hello.tar.gz
# tar解压到指定目录：
tar -xzvf hello.tar.gz -C /opt

#备注
f file 后紧跟的是目标文件名字
c compact
v visual
x express
z gz
```





## RPM

```bash
# 查询
rpm -qa | grep firefox

# 卸载
rpm -e xxxxxx
rpm -e --nodeps xxxxxx（不检查依赖）

# 安装
rpm -ivh xxxxxx.rpm
rpm -ivh --nodeps fxxxxxx.rpm（--nodeps，不检测依赖进度）

  -i       	-i=install，安装    
  -v      	-v=verbose，显示详细信息
  -h      	-h=hash，进度条      
  --nodeps	--nodeps，不检测依赖进度 
  
  # 查看状态
  sudo systemctl status clickhouse-server


```


---

## shell 常用工具





### 1. cut(看懂)

#### 1. 第2个“：”后

```shell
echo $PAHT | cut -f 2- -d ":"
# 2- 代表2后所有
```



#### 2. 切割IP地址

```shell
ifconfig | grep inet | cut -f 9- -d " " | cut -f 2 -d " "
# 就是找规律, 一点点试出来

```

### 2. sed(看懂)

#### 1. 替换某行

```shell
# 把含有GATEWAY 的行删掉, 替换成后边的. 带有引号等特殊字符时, 容易出错, 所以用删除整行, 添加整行的方式做
sudo sed -i '/GATEWAY/c\GATEWAY="192.168.10.2"' /etc/sysconfig/network-scripts/ifcfg-ens33

```

#### 2. 替换第一次匹配到的字符

```shell
# 只替换第一次匹配到的字符
sudo sed -i 's/HADOOP_HOM/HADOOP_HOME/1' /etc/profile.d/my_env.sh 
```

#### sed经典

```bash
# 把含有GATEWAY 的行删掉, 替换成后边的. 带有引号等特殊字符时, 容易出错, 所以用删除整行, 添加整行的方式做
sudo sed -i '/GATEWAY/c\GATEWAY="192.168.10.2"' /etc/sysconfig/network-scripts/ifcfg-ens33

# 只替换第一次匹配到的字符
sudo sed -i 's/HADOOP_HOM/HADOOP_HOME/1' /etc/profile.d/my_env.sh 
```





#### sed语法

```bash
-i : 直接在文件上编辑 （edit files in place）
-e [默认选项]：只在命令行输出，而文件不改变
```

#### sed例子



```bash
# 也可以查阅OneNote 搜索sed

# 替换, 传入字符串
sed -i 's/'''$old_str'''/'''$new_str'''/g' file
# s 代表替换
# g 代表全局	
# 使用g可以全局匹配, 匹配多个
# 不使用g只匹配第一个
# 使用2g会匹配第二个及第二个之后的匹配项
# 使用2只匹配第二个


# 删除第2行
sed -i '2d' test.txt

# 删除文件第二行之后的数据
sed -i '2,$d' test.txt

# 删除文件第二、三行的数据
sed -i '2,3d' test.txt

# 删除文件最后一行的数据
sed -i '$d' test.txt

# 删除不连续的行数的时候用;
sed -i '1d;3d' test.txt

# 2.1、在第二行后面插入数据append
sed -i '2a append' test.txt

# 2.2、在第二行后面插入两行的数据
sed -i '2a append\（回车）
all test' test.txt

# 2.3、在二行后面插入其他文件（append.txt）的数据
sed -i '2r append.txt' test.txt

# 3.1、替换某行的数据
sed -i '2c 替换成的值' test.txt

# 替换关键字出现的行数的数据
sed -i '/第二行/s/^/添加/g' test.txt--把'第二行'文字出现行数的数据的行首加上'添加'字符串

# 把某个关键字后面或前面添加字符串
sed -i 's/关键字/&字符串' test.txt--把字符添加到关键字后面
sed -i 's/关键字/字符串&' test.txt--把字符添加到关键字前面

```





### 3. sort(看懂)

| 选项 | 说明                     |
| ---- | ------------------------ |
| -n   | 依照数值的大小排序       |
| -r   | 以相反的顺序来排序       |
| -t   | 设置排序时所用的分隔字符 |
| -k   | 指定需要排序的列         |

#### 1. 按照“：”分割后的第三列倒序排序

```shell
sort -t : -r -k 3
```



### 4. awk(看懂)

#### 1. 切割IP

```shell
ifconfig | grep "inet " | awk '{print $2}'
# '{}' 一定是一起的
# 
```

#### 2. 将passwd文件中的用户id增加数值1并输出

```SHELL
awk -v i=1 -F : '{print $3+i}' passwd
```

#### 3. 以root关键字开头的所有行，并输出该行的第1列和第7列，中间以“，”号分割。

```shell
awk -F : '/^root/{print $1", "$2}' passwd
# '{}' 里面只需要1个print
```

#### 4. 显示文件名, 行号, 列号

```shell
awk '{print "文件名: " FILENAME "行号: " NR "列号: " NF}' awk.txt
# 比较粗暴, 直接写就行, 不用 , 分割
```







## Shell

### 输入/输出重定向
| 命令            | 功能说明                                         |
| --------------- | ------------------------------------------------ |
| command > file  | 将输出重定向到 file                              |
| command < file  | 将输入重定向到 file                              |
| command >> file | 将输出以追加的方式重定向到 file                  |
| n > file        | 将文件描述符为 n 的文件重定向到 file             |
| n >> file       | 将文件描述符为 n 的文件以追加的方式重定向到 file |
| n >& m          | 将输出文件 m 和 n 合并                           |
| n <& m          | 将输入文件 m 和 n 合并                           |
| << tag          | 将开始标记 tag 和结束标记 tag 之间的内容作为输入 |

### 脚本编辑
| 快捷方式 | 功能说明   |
| -------- | ---------- |
| shift    | 参数左移   |
| $@       | 所有的参数 |
| $#       | 参数的个数 |

## 脚本套路

```bash
# 设环境变量
#!/bin/bash
FLUME_HOME=/opt/module/flume  "注意没有空格" 

#!/bin/bash -ex。

-e，类似于在第二行写set -e其意义是Exit immediately if a command exits with a non-zero status. ；

-x 的意思是Print commands and their arguments as they are executed
```





## 脚本常用

### 脚本知识点

```bash
# 后台, nohup放在s
nohup
# 必有
#!/bin/bash
# 集群调用
ssh $host "command1; command2; #多条命令时, 必须有分号" 
# 参数作为一个整体
$*



```

### 经典命令组合

```bash
# 集群启动命令
ssh $host "command 1>f1.log 2>&1 &" # 1> 默认输出, 1可以省略, 就直接变成了> 

# 集群停止
ssh $host "ps -ef | grep file-flume-kafka | grep -v grep | awk '{print \$2}' | xargs -n1 kill" 
	# 如果进程没有启动会报错, 是正常现象
	# awk后面加\是为了转义, 命令在双引号时, 加\
	# grep -v 过滤下
# 集群状态
ssh $host "[ \$(ps -ef | grep file-flume-kafka | grep -v grep |wc -l) -eq 1 ] && echo ok || echo error" 
	# 不方便用ps查时, 常常用netstat -nltp 端口号查进程号



```

#### 待整理

```bash
#!/bin/bash
for i in hadoop102 hadoop103
do
  if [ $1 ]
  then
    echo "========== Creating log for $i =========="
    ssh $i "sed -i '/mock.date/d' /opt/module/applog/application.yml ; echo 'mock.date: \"$1\"' >> /opt/module/applog/application.yml"  # 分号很有必要，不能是空格
  fi
  ssh $i "cd /opt/module/applog; nohup java -jar gmall2020-mock-log-2021-11-29.jar 1>/dev/null 2>&1 &" 
  	# nohup 代表及时关闭其所启动的bash窗口, 进程依然执行
  	# & 后台运行
done
# 离线脚本
# GenerateLog.sh
```

#### case

```bash
case $1 in
"start")
	start
;;
"stop")
	stop
;;
*)
  	echo "Usage: f1.sh start|stop|status|restart"
;;
esac
```

#### for

```bash
for host in hadoop102 hadoop103 hadoop104
do
        command
done
```

## 配置小结

```bash
# 一般集群的配置都不会完全一样, 分发时注意
	kafka, 要改node
	kibanar, 要改node
	

```



### 经典脚本, 弄熟了, 心里就会有底

#### Sync

```bash
#!/bin/bash

#1. 判断参数个数
if [ $# -lt 1 ]
then
    echo Not Enough Arguement!
    exit;
fi

#2. 遍历集群所有机器
for host in hadoop102 hadoop103 hadoop104
do
    echo ====================  $host  ====================
    #3. 遍历所有目录，挨个发送

    for file in $@
    do
        #4. 判断文件是否存在
        if [ -e $file ]
            then
                #5. 获取父目录
                pdir=$(cd -P $(dirname $file); pwd) 
                	# -P 会进入实际的目录, 不仅仅是复制软连接

                #6. 获取当前文件的名称
                fname=$(basename $file)
                    ssh $host "mkdir -p $pdir; cd $pdir; cp $file "$file.`date +%x`.bak"; cd .." # 同步过去前, 先做个备份
                	# 当文件夹存在的时候, 有了-p, 也不会报错, 否则会报错
                rsync -av $pdir/$fname $host:$pdir
            else
                echo $file does not exists!
        fi
    done
done

```



#### SyncPro

```bash
#!/bin/bash

#1. 判断参数个数
if [ $# -lt 1 ]
then
    echo Not Enough Arguement!
    exit;
fi

#2. 遍历集群所有机器
for host in hadoop102 hadoop103 hadoop104
do
    echo ====================  $host  ====================
    #3. 遍历所有目录，挨个发送
    for file in $@
    do
        #4. 判断文件是否存在
        if [ -e $file ]
            then
                #5. 获取父目录
                pdir=$(cd -P $(dirname $file); pwd)
                #6. 获取当前文件的名称
                fname=$(basename $file)
                ssh $host "mkdir -p $pdir; cd $pdir; cp $file "$file.`date +%x`.bak"; cd .." # 同步过去前, 先做个备份
                rsync -av $pdir/$fname $host:$pdir
            else
                echo $file does not exists!
        fi
    done
done



```

### 脚本练习

#### Kafka启停脚本封装

```bash
#! /bin/bash

function kf_start(){
    for i in hadoop102 hadoop103 hadoop104
    do
        ssh $i "/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties"
        echo " --------$i Kafka Started-------"
    done
}

function kf_stop(){
    for i in hadoop102 hadoop103 hadoop104
    do
        ssh $i "/opt/module/kafka/bin/kafka-server-stop.sh "
        echo " --------$i Kafka Stoped-------"

    done
}

function kf_status(){
    for i in hadoop102 hadoop103 hadoop104
    do
        echo " --------$i Kafka-------"
        ssh $i "nc -z localhost 9092 && echo '正常' || echo '异常' "
    done
}

function list(){
    kafka-topics.sh --bootstrap-server hadoop102:9092 --list
}

case $1 in

"start")
    kf_start
;;

"stop")
    kf_stop
;;

"list")
    list
;;

"status")
    kf_status
;;

"restart")
    kf_stop
    sleep 15
    kf_start
;;

"kc")
    if [ $2 ]
        then
           kafka-console-consumer.sh --bootstrap-server hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic $2
        else
          echo "Usage: kf.sh {start|stop|kc [topic]|kp [topic]}"
        fi
;;

"kp")
    if [ $2 ]
        then
          kafka-console-producer.sh --broker-list hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic $2
        else
          echo "Usage: kf.sh {start|stop|kc [topic]|kp [topic]}"
        fi
;;


*)
    echo "Usage: kafka.sh start|start|stop|status|restart|kp TOPIC|kc TOPIC"
    ;;
esac

```
#### flume
```shell
#!/bin/bash
FLUME_HOME=/opt/module/flume

function f1_start(){
  for host in hadoop102 hadoop103
  do
    ssh $host "nohup $FLUME_HOME/bin/flume-ng agent -n a1 -c $FLUME_HOME/conf -f /opt/confs/flume/real-time-file-flume-kafka.conf -Dflume.root.logger=INFO,console 1>/opt/module/flume/f1.log 2>&1 &"
  done

}

function f1_stop(){
  for host in hadoop102 hadoop103
  do
    ssh $host "jps -l | grep  org.apache.flume.node.Application | awk '{print \$1}' | xargs -n1 kill"
  done
}

function f1_status(){
  for host in hadoop102 hadoop103
  do
    echo "==========  $host  ==========="
    # 使用jps要比用ps更好
    ssh $host "[ \$(jps -l | grep  org.apache.flume.node.Application |wc -l) -ge 1 ] && echo ok || echo error"
  done
}

case $1 in
"start")
  f1_start
  ;;
"stop")
  f1_stop
  ;;
"status")
  f1_status
  ;;
"restart")
  f1_stop
  sleep 5
  f1_start
  ;;
*)
  echo "Usage: f1.sh start|stop|status|restart"
  ;;
esac

```


#### Redis

```shell

#!/bin/bash

case "$1" in
    start)
        redis-server /opt/module/redis-6.2.1/redis.conf
        echo "redis 6379 已启动"
    ;;
    stop)
        ps -ef | grep 6379 | grep -v color | awk '{print $2}' | xargs -n1 kill -9 > /dev/null 2>&1
        echo "redis 6379 已停止" 
    ;;
      status)
        [ $(ps -ef | grep 6379  | grep -v grep |wc -l) -gt 0 ] && echo redis 6379 is ok || echo redis 6379 is down
    ;;
    *)
        echo start|stop|status
    ;;
esac


```



#### nginx

```shell
#!/bin/bash
APPNAME=gmall0906-logger-0.0.1-SNAPSHOT.jar

case "$1" in
    "start")
        ssh hadoop102 "/opt/module/nginx/sbin/nginx ;echo nginx on hadoop102 is started success"

        for item in hadoop102 hadoop103 hadoop104
        do
            echo -e "\n springboot on ${item} started success"
            ssh $item "cd /opt/module/rt_gmall/log;nohup java -jar $APPNAME > /dev/null 2>&1 &"  
        done
    ;;

    "stop")
        /opt/module/nginx/sbin/nginx -s stop

        for item in hadoop102 hadoop103 hadoop104 
        do
            echo -e "\n stopping springboot on ${item}"
            # 停止标准写法
            ssh $item "ps -ef | grep $APPNAME | grep -v grep | awk '{print \$2}' | xargs -n1 kill  > /dev/null 2>&1 &" 

        done
    ;;

    "status")

        for item in hadoop102 hadoop103 hadoop104
        do
            ssh $item "[ \$(ps -ef | grep $APPNAME | grep -v grep |wc -l) -eq 1 ] && echo springboot on $item is ok || echo springboot on $item is down" 
        done
        ssh hadoop102 "[ \$(ps -ef | grep nginx | grep -v grep |wc -l) -eq 3 ] && echo nginx is ok || echo nginx is down; echo \$(ps -ef | grep nginx | grep -v grep |wc -l)" 
        ssh hadoop102 "ps -ef | grep nginx | grep -v grep" 
    ;;

    *)
        echo "usage: rtlog_nginx_springboot.sh status|start|stop"
    ;;
esac


```




---

## Hadoop

### 启动命令

```bash
# 一键启停
sbin/start-all.sh

#群起HDFS
sbin/start-dfs.sh
#群起yarn
sbin/start-yarn.sh


#单独HDFS
bin/hdfs --daemon start namenode/datanode/secondarynamenode
#单独YARN
bin/yarn --daemon start resourcemanager/nodemanager
#单独HISTORYSERVER
bin/mapred --daemon start historyserver


# 安全模式
（1）bin/hdfs dfsadmin -safemode get	（功能描述：查看安全模式状态）
（2）bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态）
（3）bin/hdfs dfsadmin -safemode leave	（功能描述：离开安全模式状态）
（4）bin/hdfs dfsadmin -safemode wait	（功能描述：等待安全模式状态）
(5) hfds fsck / # 检测根目录健康状况
(6) hdfs fsck / -delete # 删除有问题的块

进入安全模式的几种情况, 安全模式(safeMode)就是只能读不能写, 
  1. NN磁盘空间满了或内存满了
      加磁盘或加内存
  2. 启动集群会进入安全模式
启动时候由于数据丢失无法退出安全模式
  1. 手动退出安全模式
    hdfs dfsadmin -safemode leave
  2. 删掉丢失块的文件
    hdfs fsck -delete /

安全模式应用小技巧
  1. 比如Hive框架启动时候，要求HDFS不在安全模式
  2. 我们想通过脚本上传文件到HDFS，要求HDFS不在安全模式
如果集群处在安全模式，操作会失败，可以在脚本中加入hdfs dfsadmin -safemode wait阻塞脚本执行，直到集群退出安全模式

# 黑白名单
加入后需要刷新
hdfs dfsadmin -refreshNodes


# 小文件
```



![image-20220317223437568](https://pictures-for-typora.oss-cn-beijing.aliyuncs.com/typora_imageimage-20220317223437568.png)



### hadoop fs/hdfs dfs 命令

| 功能说明                                                     | 命令                                         |
| ------------------------------------------------------------ | -------------------------------------------- |
| 创建目录                                                     | hdfs dfs -mkdir -p /data/flink               |
| 显示目录                                                     | hdfs dfs -ls /                               |
| 从HDFS拷贝到本地                                             | hdfs dfs -copyToLocal /data/data.txt ./      |
| 文件上传到集群(从本地)                                       | hhdfs dfs -copyFromLocal data.txt /          |
| 文件下载                                                     | hdfs dfs -get /data/flink                    |
| 删除集群的文件                                               | hdfs dfs -rm /data/flink                     |
| 删除文件夹                                                   | hdfs dfs -rm -r -skipTrash /data             |
| 从本地剪切粘贴到HDFS                                         | hdfs dfs  -moveFromLocal data.txt /data/     |
| 追加一个文件到已经存在的文件末尾hdfs dfs -appendToFile data1.txt /data/data.txt |                                              |
| 显示文件内容                                                 | hdfs dfs -cat data.txt                       |
| 修改文件所属权限                                             | hdfs dfs  -chmod  777 xxx.sh                 |
| 修改文件所属用户组                                           | hdfs dfs  -chown  root:root data.txt         |
| 从HDFS的一个路径拷贝到HDFS的另一个路径                       | hdfs dfs -cp data.txt /data1.txt             |
| 在HDFS目录中移动文件                                         | hdfs dfs -mv data.txt /opt/                  |
| 合并下载多个文件                                             | hdfs dfs  -getmerge /data/* ./data_merge.txt |
| hadoop fs -put                                               | 等同于copyFromLocal                          |
| 显示一个文件的末尾                                           | hdfs dfs -tail data.txt                      |
| 删除文件或文件夹                                             | hdfs dfs -rm /data/data.txt                  |
| 删除空目录                                                   | hdfs dfs -rmdir /data                        |
| 统计文件夹的大小信息                                         | hdfs dfs -s -h /data                         |
| 统计文件夹下的文件大小信息                                   | hdfs dfs  -h /data                           |
| 设置HDFS中文件的副本数量                                     | hdfs dfs -setrep 3 /data/data.txt            |


### yarn命令
| 功能说明                   | 命令                           |
| -------------------------- | ------------------------------ |
| 查看正在运行的yarn任务列表 | yarn application -list appID   |
| kill掉指定id的yarn任务     | yarn application -kill appID   |
| 查看任务日志信息           | yarn logs -applicationId appID |

---





## Zookeeper

![image-20220316223108304](https://pictures-for-typora.oss-cn-beijing.aliyuncs.com/typora_imageimage-20220316223108304.png)



###  启动命令
| 功能说明            | 命令脚本                        |
| ------------------- | ------------------------------- |
| 启动zookeeper服务   | zkServer.sh start               |
| 查看zookeeper状态   | zkServer.sh status              |
| 停止zookeeper服务   | zkServer.sh stop                |
| 启动zookeeper客户端 | zkCli.sh -server 127.0.0.1:2181 |
| 退出zookeeper客户端 | quit                            |

###  基本操作
| 功能说明                                  | 命令脚本                      |
| ----------------------------------------- | ----------------------------- |
| 当前znode中所包含的内容                   | ls /                          |
| 创建普通节点(前面是节点的路径，后面是值） | create /bigdata/flink "flink" |
| 获取节点的值                              | get /bigdata                  |
| 修改节点的值                              | set /bigdata/flink "flinksql" |
| 删除节点                                  | delete /bigdata/flink         |
| 递归删除节点                              | rmr /bigdata                  |

### 四字母命令

| 命令 | 功能说明                                                     | 例子                           |
| ---- | ------------------------------------------------------------ | ------------------------------ |
| conf | zk服务配置的详细信息                                         | echo conf \| nc 127.0.0.1 2181 |
| stat | 客户端与zk连接的简要信息                                     | 参考上面                       |
| srvr | zk服务的详细信息                                             | 参考上面                       |
| cons | 客户端与zk连接的详细信息                                     | 参考上面                       |
| mntr | zk服务目前的性能状况                                         | 参考上面                       |
| crst | 重置当前的所有连接、会话                                     | 参考上面                       |
| dump | 列出未经处理的会话和连接信息                                 | 参考上面                       |
| envi | 列出zk的版本信息、主机名称、Java版本、服务器名称等等         | 参考上面                       |
| ruok | 测试服务器是否正在运行，如果在运行返回imok，否则返回空       | 参考上面                       |
| srst | 重置Zookeeper的所有统计信息                                  | 参考上面                       |
| wchs | 列出watch的总数，连接数                                      | 参考上面                       |
| wchp | 列出所有watch的路径及sessionID                               | 参考上面                       |
| mntr | 列出集群的关键性能数据，包括zk的版本、node数量、临时节点数等等 | 参考上面                       |


---

## Kafka
### 练习

##### 1. 所有集群都关闭的情况下, 手动一个个启动下, 看能否正确启动

```bash
1. zookeeper启动
2. hdfs启动
3. 参数不能提示, 最好复制
```





### 查看当前服务器中的所有topic

~~~shell
kafka-topics.sh --bootstrap-server hadoop102:9092 --list
~~~
### 创建topic

~~~shell
kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 1 --replication-factor 3 --topic first
~~~

### 删除topic

**注意:** 需要server.properties中设置delete.topic.enable=true否则只是标记删除
~~~shell
kafka-topics --zookeeper xxxxxx:2181 --delete --topic topic_name
~~~

### 查看某个Topic的详情

```shel
kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first
```

### 修改分区数, 不能修改副本数

```shell
kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions
```

### 删除topic

```shell
kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic first
```

### 生产者

~~~shell
kafka-console-producer --broker-list xxxxxx:9092 --topic topic_name

可加：--property parse.key=true（有key消息）
~~~
### 消费者

~~~shell
kafka-console-consumer --bootstrap-server xxxxxx:9092 --topic topic_name

注：可选

--from-beginning：会把主题中以往所有的数据都读取出来

--whitelist '.*' ：消费所有的topic

--property print.key=true：显示key进行消费

--partition 0：指定分区消费

--offset：指定起始偏移量消费
~~~

### 查看某个Topic的详情
~~~shell
kafka-topics --zookeeper xxxxxx:2181 --describe --topic topic_name
~~~

### 修改分区数 
~~~shell
kafka-topics --zookeeper xxxxxx:2181 --alter --topic topic_name --partitions 6
~~~

### 查看某个消费者组信息
~~~shell
kafka-consumer-groups --bootstrap-server  xxxxxx:9092  --describe --group group_name 
~~~
### 删除消费者组
~~~shell
kafka-consumer-groups --bootstrap-server  xxxxxx:9092  ---delete --group group_name 
~~~

### 重置offset
~~~shell
kafka-consumer-groups --bootstrap-server  xxxxxx:9092  --group group_name

--reset-offsets --all-topics --to-latest --execute 
~~~

### leader重新选举

指定Topic指定分区用重新PREFERRED：优先副本策略 进行Leader重选举
~~~shell
kafka-leader-election --bootstrap-server xxxxxx:9092 
--topic topic_name --election-type PREFERRED --partition 0
~~~

所有Topic所有分区用重新PREFERRED：优先副本策略 进行Leader重选举
~~~shell
kafka-leader-election --bootstrap-server xxxxxx:9092 
--election-type preferred  --all-topic-partitions
~~~

### 查询kafka版本信息
~~~shell
kafka-configs --bootstrap-server xxxxxx:9092
--describe --version
~~~
### 增删改配置

| 功能说明      | 参数                                                         |
| ------------- | ------------------------------------------------------------ |
| 选择类型      | --entity-type (topics/clients/users/brokers/broker- loggers) |
| 类型名称      | --entity-name                                                |
| 删除配置      | --delete-config k1=v1,k2=v2                                  |
| 添加/修改配置 | --add-config k1,k2                                           |

topic添加/修改动态配置
~~~shell
kafka-configs --bootstrap-server xxxxxx:9092
--alter --entity-type topics --entity-name topic_name 
--add-config file.delete.delay.ms=222222,retention.ms=999999
~~~

topic删除动态配置
~~~shell
kafka-configs --bootstrap-server xxxxxx:9092 
--alter --entity-type topics --entity-name topic_name 
--delete-config file.delete.delay.ms,retention.ms
~~~

### 持续批量拉取消息

单次最大消费10条消息(不加参数意为持续消费)
~~~shell
kafka-verifiable-consumer --bootstrap-server xxxxxx:9092 
--group group_name
--topic topic_name --max-messages 10
~~~

### 删除指定分区的消息

删除指定topic的某个分区的消息删除至offset为1024

json文件offset-json-file.json
~~~
{
    "partitions": [
        {
            "topic": "topic_name",
            "partition": 0,
            "offset": 1024
        }
    ],
    "version": 1
}
~~~

~~~shell
kafka-delete-records --bootstrap-server xxxxxx:9092 
--offset-json-file offset-json-file.json
~~~

### 查看Broker磁盘信息

查询指定topic磁盘信息
~~~shell
kafka-log-dirs --bootstrap-server xxxxxx:9090 
--describe --topic-list topic1,topic2
~~~

查询指定Broker磁盘信息
~~~shell
kafka-log-dirs --bootstrap-server xxxxxx:9090 
--describe --topic-list topic1 --broker-list 0
~~~

## Mysql

### 使用步骤

#### 卸载

```bash
#!/bin/bash
service mysql stop 2>/dev/null
service mysqld stop 2>/dev/null
rpm -qa | grep -i mysql | xargs -n1 rpm -e --nodeps 2>/dev/null
rpm -qa | grep -i mariadb | xargs -n1 rpm -e --nodeps 2>/dev/null
rm -rf /var/lib/mysql
rm -rf /var/log/mysqld.log
rm -rf /usr/lib64/mysql
rm -rf /etc/my.cnf
rm -rf /usr/my.cnf
```

安装

```bash

```

配置

```bash

```

安装

```bash

```

安装

```bash

```



## Hive

### 启动类
| 功能说明            | 命令                                           |
| ------------------- | ---------------------------------------------- |
| 启动hiveserver2服务 | bin/hiveserver2                                |
| 启动beeline         | bin/beeline                                    |
| 连接hiveserver2     | beeline> !connect jdbc:hive2://hadoop102:10000 |
| metastroe服务       | bin/hive --service metastore                   |

hive 启动元数据服务（metastore和hiveserver2）和优雅关闭脚本
~~~
启动： hive.sh start
关闭： hive.sh stop
重启： hive.sh restart
状态： hive.sh status
~~~
脚本如下
~~~bash
#!/bin/bash
HIVE_LOG_DIR=$HIVE_HOME/logs
mkdir -p $HIVE_LOG_DIR
#检查进程是否运行正常，参数1为进程名，参数2为进程端口
function check_process()
{
    pid=$(ps -ef 2>/dev/null | grep -v grep | grep -i $1 | awk '{print $2}')
    ppid=$(netstat -nltp 2>/dev/null | grep $2 | awk '{print $7}' | cut -d '/' -f 1)
    echo $pid
    [[ "$pid" =~ "$ppid" ]] && [ "$ppid" ] && return 0 || return 1
}

function hive_start()
{
    metapid=$(check_process HiveMetastore 9083)
    cmd="nohup hive --service metastore >$HIVE_LOG_DIR/metastore.log 2>&1 &"
    cmd=$cmd" sleep4; hdfs dfsadmin -safemode wait >/dev/null 2>&1"
    [ -z "$metapid" ] && eval $cmd || echo "Metastroe服务已启动"
    server2pid=$(check_process HiveServer2 10000)
    cmd="nohup hive --service hiveserver2 >$HIVE_LOG_DIR/hiveServer2.log 2>&1 &"
    [ -z "$server2pid" ] && eval $cmd || echo "HiveServer2服务已启动"
}

function hive_stop()
{
    metapid=$(check_process HiveMetastore 9083)
    [ "$metapid" ] && kill $metapid || echo "Metastore服务未启动"
    server2pid=$(check_process HiveServer2 10000)
    [ "$server2pid" ] && kill $server2pid || echo "HiveServer2服务未启动"
}

case $1 in
"start")
    hive_start
    ;;
"stop")
    hive_stop
    ;;
"restart")
    hive_stop
    sleep 2
    hive_start
    ;;
"status")
    check_process HiveMetastore 9083 >/dev/null && echo "Metastore服务运行正常" || echo "Metastore服务运行异常"
    check_process HiveServer2 10000 >/dev/null && echo "HiveServer2服务运行正常" || echo "HiveServer2服务运行异常"
    ;;
*)
    echo Invalid Args!
    echo 'Usage: '$(basename $0)' start|stop|restart|status'
    ;;
esac
~~~

### 常用交互命令

| 功能说明                    | 命令                  |
| --------------------------- | --------------------- |
| 不进入hive的交互窗口执行sql | bin/hive -e "sql语句" |
| 执行脚本中sql语句           | bin/hive -f hive.sql  |
| 退出hive窗口                | exit 或 quit          |
| 命令窗口中查看hdfs文件系统  | dfs -ls /             |
| 命令窗口中查看hdfs文件系统  | ! ls /data/h          |

### SQL类(特殊的)

| 说明                   | 语句                                                         |
| ---------------------- | ------------------------------------------------------------ |
| 查看hive中的所有数据库 | show databases                                               |
| 用default数据库        | use default                                                  |
| 查询表结构             | desc table_name                                              |
| 查看数据库             | show databases                                               |
| 重命名表名             | alter table table1 rename to table2                          |
| 修改表中字段           | alter table table_name change name user_name String          |
| 修改字段类型           | alter table table_name change salary salary Double           |
| 创建外部表             | create external table ....                                   |
| 查询外部表信息         | desc formatted outsidetable                                  |
| 创建视图               | create view view_name as select * from table_name .....      |
| 添加数据               | load data local inpath 'xxx'  overwrite into table table_name partition(day='2021-12-01') |

### 内置函数

（1） NVL

给值为NULL的数据赋值，它的格式是NVL( value，default_value)。它的功能是如果value为NULL，则NVL函数返回default_value的值，否则返回value的值，如果两个参数都为NULL ，则返回NULL

~~~
select nvl(column, 0) from xxx；
~~~

（2）行转列

| 函数                                 | 描述                                                         |
| ------------------------------------ | ------------------------------------------------------------ |
| CONCAT(string A/col, string B/col…)  | 返回输入字符串连接后的结果，支持任意个输入字符串             |
| CONCAT_WS(separator, str1, str2,...) | 第一个参数参数间的分隔符，如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间。 |
| COLLECT_SET(col)                     | 将某字段的值进行去重汇总，产生array类型字段                  |
| COLLECT_LIST(col)                    | 函数只接受基本数据类型，它的主要作用是将某字段的值进行不去重汇总，产生array类型字段。 |

（3）列转行(一列转多行)

**Split(str, separator)：** 将字符串按照后面的分隔符切割，转换成字符array。

**EXPLODE(col)：**
将hive一列中复杂的array或者map结构拆分成多行。

**LATERAL VIEW**
~~~
用法：

LATERAL VIEW udtf(expression) tableAlias AS columnAlias
~~~
解释：lateral view用于和split, explode等UDTF一起使用，它能够将一行数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。

lateral view首先为原始表的每行调用UDTF，UDTF会把一行拆分成一或者多行，lateral view再把结果组合，产生一个支持别名表的虚拟表。

**准备数据源测试**

| movie     | category       |
| --------- | -------------- |
| 《功勋》  | 记录,剧情      |
| 《战狼2》 | 战争,动作,灾难 |

**SQL**

~~~
SELECT movie,category_name 
FROM movie_info 
lateral VIEW
explode(split(category,",")) movie_info_tmp  AS category_name ;

~~~

**测试结果**
~~~
《功勋》      记录
《功勋》      剧情
《战狼2》     战争
《战狼2》     动作
《战狼2》     灾难

~~~

### 窗口函数

（1）OVER()

定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化。

（2）CURRENT ROW（当前行）

~~~
n PRECEDING：往前n行数据

n FOLLOWING：往后n行数据
~~~

（3）UNBOUNDED（无边界）

~~~
UNBOUNDED PRECEDING 前无边界，表示从前面的起点

UNBOUNDED FOLLOWING后无边界，表示到后面的终点
~~~

**SQL案例：由起点到当前行的聚合**

~~~
select 
    sum(money) over(partition by user_id order by pay_time rows between UNBOUNDED PRECEDING and current row) 
from or_order;
~~~

**SQL案例：当前行和前面一行做聚合**

~~~
select 
    sum(money) over(partition by user_id order by pay_time rows between 1 PRECEDING and current row) 
from or_order;
~~~

**SQL案例：当前行和前面一行和后一行做聚合**

~~~
select 
    sum(money) over(partition by user_id order by pay_time rows between 1 PRECEDING AND 1 FOLLOWING )
from or_order;
~~~

**SQL案例：当前行及后面所有行**
~~~
select 
    sum(money) over(partition by user_id order by pay_time rows between current row and UNBOUNDED FOLLOWING  )
from or_order;
~~~

（4）LAG(col,n,default_val)

往前第n行数据，没有的话default_val

（5）LEAD(col,n, default_val)

往后第n行数据，没有的话default_val

**SQL案例：查询用户购买明细以及上次的购买时间和下次购买时间**

~~~
select 
	user_id,,pay_time,money,
	
	lag(pay_time,1,'1970-01-01') over(PARTITION by name order by pay_time) prev_time,
	
	lead(pay_time,1,'1970-01-01') over(PARTITION by name order by pay_time) next_time
from or_order;

~~~

（6）FIRST_VALUE(col,true/false)

当前窗口下的第一个值，第二个参数为true，跳过空值。

（7）LAST_VALUE (col,true/false)

当前窗口下的最后一个值，第二个参数为true，跳过空值。

**SQL案例：查询用户每个月第一次的购买时间 和 每个月的最后一次购买时间**

~~~
select
	FIRST_VALUE(pay_time) 
	    over(
	        partition by user_id,month(pay_time) order by pay_time 
	        rows between UNBOUNDED PRECEDING and UNBOUNDED FOLLOWING
	        ) first_time,
	
	LAST_VALUE(pay_time) 
	    over(partition by user_id,month(pay_time) order by pay_time rows between UNBOUNDED PRECEDING and UNBOUNDED FOLLOWING
	    ) last_time
from or_order;

~~~


（8）NTILE(n)

把有序窗口的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。（用于将分组数据按照顺序切分成n片，返回当前切片值）

**SQL案例：查询前25%时间的订单信息**

~~~
select * from (
    select User_id,pay_time,money,
    
    ntile(4) over(order by pay_time) sorted
    
    from or_order
) t
where sorted = 1;

~~~

### 4个By
（1）Order By

全局排序，只有一个Reducer。

（2）Sort By

分区内有序。

（3）Distrbute By

类似MR中Partition，进行分区，结合sort by使用。

（4） Cluster By

当Distribute by和Sorts by字段相同时，可以使用Cluster by方式。Cluster by除了具有Distribute by的功能外还兼具Sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。

在生产环境中Order By用的比较少，容易导致OOM。

在生产环境中Sort By+ Distrbute By用的多。

### 排序函数

（1）RANK() 

排序相同时会重复，总数不会变
~~~
1
1
3
3
5
~~~
（2）DENSE_RANK() 

排序相同时会重复，总数会减少
~~~
1
1
2
2
3
~~~
（3）ROW_NUMBER()

会根据顺序计算
~~~
1
2
3
4
5
~~~

### 日期函数
datediff：返回结束日期减去开始日期的天数
~~~
datediff(string enddate, string startdate) 

select datediff('2021-11-20','2021-11-22') 
~~~

date_add：返回开始日期startdate增加days天后的日期
~~~
date_add(string startdate, int days) 

select date_add('2021-11-20',3) 
~~~
date_sub：返回开始日期startdate减少days天后的日期

~~~bash
date_sub (string startdate, int days) 

select date_sub('2021-11-22',3)

~~~

### 启动脚本

#### Beeline

```bash
#!/bin/bash
beeline -u jdbc:hive2://hadoop102:10000 -n atguigu --hiveconf hive.exec.mode.local.auto=true

```



## hbase命令demo

```sql
1. HBase shell namespace 
   1) 查看所有的namespace
      list_namespace
   2) 创建namespace
      create_namespace 'mydb' 
      create_namespace 'mydb1' , {'createtime'=>'2022-05-31' , 'author'=>'atguigu'}   
   3) 查看namespace的详情
      describe_namespace 'mydb'
   4) 修改namespace
      alter_namespace 'mydb1' , {METHOD=>'set' , 'author'=>'wyh' }
      alter_namespace 'mydb1' , {METHOD=>'set' , 'addr'=>'beijing'}
      alter_namespace 'mydb1' , {METHOD=>'set' , 'author'=>'wyh' ,'addr'=>'beijing'}
      alter_namespace 'mydb1' , {METHOD=>'unset' , NAME=>'addr' }
   5) 删除namespace(只能删除空的namespace)
      drop_namespace 'mydb'   
   6) 查看namespace下的表
      list_namespace_tables 'mydb1'   
         
2. HBase shell table ddl 
   
   1) 查看所有的表, 默认
      list

   2) 创建表
      create 'test1', {NAME=>'info1' , VERSIONS=>'3'} , {NAME=>'info2'}
      create 'my1:test2' , {NAME=>'info1'}
      create 'test3' , 'info1' , 'info2'

   3) 查看表的详情
      describe 'test1'
      desc 'test1'

   4) 修改表
      alter 'test3' , NAME=> 'info1' , VERSIONS => 5
      alter 'test3' , NAME=> 'info3' , VERSIONS => 5
      alter 'test3',  NAME => 'info1', METHOD => 'delete'
      alter 'test3', 'delete' => 'info2'
   
   5) 表的状态
      is_enabled 'test3'
      is_disabled 'test3'
      disable 'test3'
      enable 'test3'

   6) 删除表
      disable 'test3'
      drop 'test3'   

   7) 查看表是否存在
      exists 'test3'  

   8) 查看表的regions
      list_regions 'test3'     
   
3. HBase shell table dml 
   
   1) 新增数据
      disable 'stu'
      drop 'stu'
      create 'stu' , 'info1' , 'info2'
      put 'stu' , '1001' , 'info1:name' , 'zhangsan'
      put 'stu' , '1001' , 'info1:age'  , '20'
      put 'stu' , '1001' , 'info1:gender' , 'man'
      put 'stu' , '1001' , 'info2:addr' , 'beijing'
      
      put 'stu' , '1002' , 'info1:name' , 'lisi'
      put 'stu' , '1002' , 'info1:age'  , '25'
      put 'stu' , '1002' , 'info1:gender' , 'woman'
      put 'stu' , '1002' , 'info2:addr' , 'shanghai'
      put 'stu' , '1002' , 'info2:passwd' , 'nicai'
      
      put 'stu' , '1003' , 'info1:name' , 'wangwu'
      put 'stu' , '1003' , 'info1:age'  , '30'
      put 'stu' , '1003' , 'info1:gender' , 'man'
      put 'stu' , '1003' , 'info2:addr' , 'tianjing'

      put 'stu' , '10021' , 'info1:name' , 'zhaoliu'
      put 'stu' , '10021' , 'info1:age'  , '35'
      put 'stu' , '10021' , 'info1:gender' , 'man'
      put 'stu' , '10021' , 'info2:addr' , 'hebei'

      put 'stu' , '10020' , 'info1:name' , 'tianqi'
      put 'stu' , '10020' , 'info1:age'  , '40'
      put 'stu' , '10020' , 'info1:gender' , 'women'
      put 'stu' , '10020' , 'info2:addr' , 'shanxi'


   2) 基于rowkey查询数据
      get 'stu' , '1001'    
      get 'stu' , '1001' , 'info1:name'

   3) 扫描数据
      scan 'stu'   
      scan 'stu' ,{STARTROW=>'1001'  , STOPROW=> '1002!'}
      scan 'stu' ,{STARTROW=>'1001'  , STOPROW=> '1002|'}
      scan 'stu' , {RAW=>true, VERSIONS=>5}

   4) 修改数据
      put 'stu' , '1001' , 'info1:name' ,'zhangxiaosan'

   5) 删除操作
      delete 'stu' , '1001' , 'info1:name'  删除指定版本的数据，默认最新版本(Delete) 
      deleteall 'stu' , '1002' , 'info1:name' 删除一个列所有版本的数据(DeleteColumn)
      deleteall 'stu' , '1002' 删除整个列族的数据(DeleteFamily)  
   6) 统计表中的数据条数
      count 'stu' 
   7) 清空表数据
      truncate 'stu'  直接删除内存中对应的数据和HDFS中对应的数据文件 





         

```




---

## Redis

### 约定

1.   成功返回1, 不成功返回0
2.   计算时, 成功, 返回当时的值
3.   ttl, -1永不过期, -2已过期, 其他正数, 代表还剩下的描述
4.   range, 从0开始, 左闭右闭. -1 代表最后一个元素
5.   

### key
| 命令                   | 功能说明                                     |
| ---------------------- | -------------------------------------------- |
| keys  *                | 查看当前库的所有键                           |
| exists <key>           | 判断某个键是否存在                           |
| type <key>             | 查看键的类型                                 |
| del <key>              | 删除某个键                                   |
| expire <key> <seconds> | 为键值设置过期时间，单位秒                   |
| ttl <key>              | 查看还有多久过期,-1表示永不过期,-2表示已过期 |
| dbsize                 | 查看当前数据库中key的数量                    |
| flushdb                | 清空当前库                                   |
| Flushall               | 通杀全部库                                   |

### String
| 命令                                   | 功能说明                                                     |
| -------------------------------------- | ------------------------------------------------------------ |
| get <key>                              | 查询对应键值                                                 |
| set <key> <value>                      | 添加键值对                                                   |
| append <key> <value>                   | 将给定的<value>追加到原值的末尾                              |
| strlen <key>                           | 获取值的长度                                                 |
| setnx <key> <value>                    | 只有在key 不存在时设置key的值                                |
| incr <key>                             | 将key中存储的数字值增1只能对数字值操作，如果为空，新增值为1  |
| decr <key>                             | 将key中存储的数字值减1只能对数字之操作，如果为空,新增值为-1  |
| incrby /decrby <key> 步长              | 将key中存储的数字值增减，自定义步长                          |
| mset <key1> <value1> <key2> <value2>   | 同时设置一个或多个key-value对                                |
| mget <key1> <key2>  <key3>             | 同时获取一个或多个value                                      |
| msetnx <key1> <value1> <key2> <value2> | 同时设置一个或多个key-value对，当且仅当所有给定的key都不存在 |
| getrange <key> <起始位置> <结束位置>   | 获得值的范围,类似java中的substring                           |
| setrange <key> <起始位置> <value>      | 用<value>覆盖<key>所存储的字符串值，从<起始位置>开始         |
| setex <key> <过期时间> <value>         | 设置键值的同时，设置过去时间，单位秒                         |
| getset <key> <value>                   | 以新换旧,设置了新值的同时获取旧值                            |

### List
| 命令                                      | 功能说明                                       |
| ----------------------------------------- | ---------------------------------------------- |
| lpush/rpush  <key>  <value1>  <value2>    | 从左边/右边插入一个或多个值。                  |
| lpop/rpop  <key>                          | 从左边/右边吐出一个值。值在键在，值光键亡。    |
| rpoplpush  <key1>  <key2>                 | 从<key1>列表右边吐出一个值，插到<key2>列表左边 |
| lrange <key> <start> <stop>               | 按照索引下标获得元素(从左到右)                 |
| lindex <key> <index>                      | 按照索引下标获得元素(从左到右)                 |
| llen <key>                                | 获得列表长度                                   |
| linsert <key>  before <value>  <newvalue> | 在<value>的后面插入<newvalue> 插入值           |
| lrem <key> <n>  <value>                   | 从左边删除n个value(从左到右)                   |

### Set
| 命令                                | 功能说明                                                     |
| ----------------------------------- | ------------------------------------------------------------ |
| sadd <key>  <value1>  <value2> .... | 将一个或多个 member 元素加入到集合 key 当中，已经存在于集合的 member 元素将被忽略。 |
| smembers <key>                      | 取出该集合的所有值。                                         |
| sismember <key>  <value>            | 判断集合<key>是否为含有该<value>值，有返回1，没有返回0       |
| scard  <key>                        | 返回该集合的元素个数。                                       |
| srem <key> <value1> <value2> ....   | 删除集合中的某个元素。                                       |
| spop <key>                          | 随机从该集合中吐出一个值, 会删除 取出来的元素.               |
| srandmember <key> <n>               | 随机从该集合中取出n个值。不会从集合中删除                    |
| sinter <key1> <key2>                | 返回两个集合的交集元素。                                     |
| sunion <key1> <key2>                | 返回两个集合的并集元素。                                     |
| sunionstore <des> <source>          | 复制整个集合, 加上sotre之后都可以存起来                      |
| sdiff <key1> <key2>                 | 返回两个集合的差集元素。                                     |



### Hash
| 命令                                                 | 功能说明                                                     |
| ---------------------------------------------------- | ------------------------------------------------------------ |
| hset <key>  <field>  <value>                         | 给<key>集合中的  <field>键赋值<value>                        |
| hget <key1>  <field>                                 | 从<key1>集合<field> 取出 value                               |
| hgetall                                              |                                                              |
| hmset <key1>  <field1> <value1> <field2> <value2>... | 批量设置hash的值                                             |
| hexists key  <field>                                 | 查看哈希表 key 中，给定域 field 是否存在。                   |
| hkeys <key>                                          | 列出该hash集合的所有field                                    |
| hvals <key>                                          | 列出该hash集合的所有value                                    |
| hincrby <key> <field>  <increment>                   | 为哈希表 key 中的域 field 的值加上增量 increment             |
| hsetnx <key>  <field> <value>                        | 将哈希表 key 中的域 field 的值设置为 value ，当且仅当域 field 不存在 |

### zset(Sorted set)
| 命令                                                         | 功能说明                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| zadd  <key> <score1> <value1>  <score2> <value2>...          | 将一个或多个 member 元素及其 score 值加入到有序集 key 当中   |
| zrange <key>  <start> <stop>  [WITHSCORES]                   | 返回有序集 key 中，下标在<start> <stop>之间的元素带WITHSCORES，可以让分数一起和值返回到结果集。 |
| zrangebyscore key min max [withscores] [limit offset count]  | 返回有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。有序集成员按 score 值递增(从小到大)次序排列。 |
| zrevrangebyscore key max min [withscores] [limit offset count] | 同上，改为从大到小排列。                                     |
| zincrby <key> <increment> <value>                            | 为元素的score加上增量                                        |
| zrem  <key>  <value>                                         | 删除该集合下，指定值的元素                                   |
| zcount <key>  <min>  <max>                                   | 统计该集合，分数区间内的元素个数                             |
| zrank <key>  <value>                                         | 返回该值在集合中的排名，从0开始。                            |


---


## Flink
### 启动
~~~bash
./start-cluster.sh 
~~~

### run
~~~bash
./bin/flink run [OPTIONS]

./bin/flink run -m yarn-cluster -c com.wang.flink.WordCount /opt/app/WordCount.jar
~~~

| OPTIONS | 功能说明                                               |
| ------- | ------------------------------------------------------ |
| -d      | detached 是否使用分离模式                              |
| -m      | jobmanager 指定提交的jobmanager                        |
| -yat    | –yarnapplicationType 设置yarn应用的类型                |
| -yD     | 使用给定属性的值                                       |
| -yd     | –yarndetached 使用yarn分离模式                         |
| -yh     | –yarnhelp yarn session的帮助                           |
| -yid    | –yarnapplicationId 挂到正在运行的yarnsession上         |
| -yj     | –yarnjar Flink jar文件的路径                           |
| -yjm    | –yarnjobManagerMemory jobmanager的内存(单位M)          |
| -ynl    | –yarnnodeLabel 指定 YARN 应用程序 YARN 节点标签        |
| -ynm    | –yarnname 自定义yarn应用名称                           |
| -yq     | –yarnquery 显示yarn的可用资源                          |
| -yqu    | –yarnqueue 指定yarn队列                                |
| -ys     | –yarnslots 指定每个taskmanager的slots数                |
| -yt     | yarnship 在指定目录中传输文件                          |
| -ytm    | –yarntaskManagerMemory 每个taskmanager的内存           |
| -yz     | –yarnzookeeperNamespace 用来创建ha的zk子路径的命名空间 |
| -z      | –zookeeperNamespace 用来创建ha的zk子路径的命名空间     |
| -p      | 并行度                                                 |
| -yn     | 需要分配的YARN容器个数(=任务管理器的数量)              |

### info
~~~
./bin/flink info [OPTIONS]
~~~
| OPTIONS | 功能说明         |
| ------- | ---------------- |
| -c      | 程序进入点，主类 |
| -p      | 并行度           |


### list
~~~
./bin/flink list [OPTIONS]
~~~
| OPTIONS | 功能说明                                                 |
| ------- | -------------------------------------------------------- |
| -a      | –all 显示所有应用和对应的job id                          |
| -r      | –running 显示正在运行的应用和job id                      |
| -s      | –scheduled 显示调度的应用和job id                        |
| -m      | –jobmanager 指定连接的jobmanager                         |
| -yid    | –yarnapplicationId 挂到指定的yarn id对应的yarn session上 |
| -z      | –zookeeperNamespace 用来创建ha的zk子路径的命名空间       |

### stop
~~~
./bin/flink stop  [OPTIONS] <Job ID>
~~~
| OPTIONS | 功能说明                                                 |
| ------- | -------------------------------------------------------- |
| -d      | 在采取保存点和停止管道之前，发送MAX_WATERMARK            |
| -p      | savepointPath 保存点的路径 'xxxxx'                       |
| -m      | –jobmanager 指定连接的jobmanager                         |
| -yid    | –yarnapplicationId 挂到指定的yarn id对应的yarn session上 |
| -z      | –zookeeperNamespace 用来创建ha的zk子路径的命名空间       |

### cancel(弱化)
~~~
./bin/flink cancel  [OPTIONS] <Job ID>
~~~
| OPTIONS | 功能说明                                                 |
| ------- | -------------------------------------------------------- |
| -s      | 使用 "stop "代替                                         |
| -D      | 允许指定多个通用配置选项                                 |
| -m      | 要连接的JobManager的地址                                 |
| -yid    | –yarnapplicationId 挂到指定的yarn id对应的yarn session上 |
| -z      | –zookeeperNamespace 用来创建ha的zk子路径的命名空间       |

### savepoint

~~~
./bin/flink savepoint  [OPTIONS] <Job ID>
~~~
| OPTIONS | 功能说明                                                 |
| ------- | -------------------------------------------------------- |
| -d      | 要处理的保存点的路径                                     |
| -j      | Flink程序的JAR文件                                       |
| -m      | 要连接的JobManager的地址                                 |
| -yid    | –yarnapplicationId 挂到指定的yarn id对应的yarn session上 |
| -z      | –zookeeperNamespace 用来创建ha的zk子路径的命名空间       |

## ElasticSearch

```bash
ElasticSearch 本来立意就高, 要处理大量数据. 所以下面的配置不更改, 根本就起不来

sudo vim /etc/security/limits.conf
#在文件最后添加如下内容:
* soft nofile 65536
* hard nofile 131072
* soft nproc 2048
* hard nproc 65536

sudo vim /etc/sysctl.conf  
#在文件最后添加如下内容
vm.max_map_count=262144
 
sudo vim /etc/security/limits.d/20-nproc.conf   
#修改如下内容
* soft nproc 65536

'注意分发, 分发完后看一眼'
```



## clickhouse

```
$ mkdir clickhouse
$ cd clickhouse
$ curl -O 'https://builds.clickhouse.com/master/amd64/clickhouse' && chmod a+x ./clickhouse
$ ./clickhouse server
```

再启动一个终端窗口，运行

```
$ ./clickhouse client
```



### 面试问题

### merge tree

1. 在内存中将数据块排序
2. 追加写入硬盘
3. optimize: 按照归并排序合并数据块

### replacing merge tree

1. 在内存中将数据块排序并<span style="color:red">去重</span>
2. 追加写入硬盘
3. optimize: 归并排序+去重

### summing merge tree

1. 在内存中将数据块排序并<span style="color:red">聚合</span>
2. 追加写入硬盘
3. optimize: 归并排序+聚合



分布式一致性：

- paxos算法
    - zookeeper中的zab协议
    - raft算法（k8s的etcd，kafka 2.8之后的版本）

《数据密集型应用系统设计》（ddia）



## kibanar

```bash
#!/bin/bash 
es_home=/opt/module/es7
kibana_home=/opt/module/kibana7
if [ $# -lt 1 ]
then
	echo "USAGE:es.sh {start|stop}"
	exit
fi	
case $1  in
"start") 
  #启动ES		
  for i in hadoop102 hadoop103 hadoop104
  do
    	ssh $i  "source /etc/profile;nohup ${es_home}/bin/elasticsearch >/dev/null 2>&1 &"
  done
  #启动Kibana
  ssh hadoop102 "nohup ${kibana_home}/bin/kibana > ${kibana_home}/logs/kibana.log 2>&1 &"
;;
"stop") 
  #停止Kibana
  ssh hadoop102 "sudo netstat -nltp | grep 5601 |grep -v tcp|awk '{print $7}' | awk -F /  '{print \$1}'| xargs kill" 
  #停止ES
  for i in hadoop102 hadoop103 hadoop104
  do
      ssh $i "ps -ef|grep $es_home |grep -v grep|awk '{print \$2}'|xargs kill" >/dev/null 2>&1
  done
 
;;
*)
	echo "USAGE:es.sh {start|stop}"
	exit
;;
esac



```

## pom

#### 常用依赖

```xml
<dependencies>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.1.3</version>
    </dependency>
    
    <!-- 单元测试 -->
    <dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <version>4.12</version>
    </dependency>
        <dependency>
            <groupId>org.hamcrest</groupId>
            <artifactId>hamcrest-core</artifactId>
            <version>1.3</version>
        </dependency>
    
    <!-- 禁止log4j输出 -->
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-nop</artifactId>
            <version>1.7.2</version>
        </dependency>
    
    
</dependencies>
```

#### hadoop

```xml
```

#### 去掉log4j报错

新建log4j

```xml
log4j.rootCategory=ERROR,console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/ddHH:mm:ss}%p%c{1}:%m%n

#Setthedefaultspark-shelllogleveltoERROR.Whenrunningthespark-shell,the
#loglevelforthisclassisusedtooverwritetherootlogger'sloglevel,sothat
#theusercanhavedifferentdefaultsfortheshellandregularSparkapps.
log4j.logger.org.apache.spark.repl.Main=ERROR

#Settingstoquietthirdpartylogsthataretooverbose
log4j.logger.org.spark_project.jetty=ERROR
log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ERROR
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ERROR
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR

#SPARK-9183:SettingstoavoidannoyingmessageswhenlookingupnonexistentUDFsinSparkSQLwithHivesupport
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
```



#### kafka

```xml
     <dependency>
          <groupId>org.apache.kafka</groupId>
          <artifactId>kafka-clients</artifactId>
          <version>3.0.0</version>
     </dependency>
```



#### 打包带有依赖

```xml
<build>
    <plugins>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
        <configuration>
          <source>1.8</source>
          <target>1.8</target>
        </configuration>
      </plugin>
      <plugin>
        <artifactId>maven-assembly-plugin</artifactId>
        <configuration>
          <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
          </descriptorRefs>
        </configuration>
        <executions>
          <execution>
            <id>make-assembly</id>
            <phase>package</phase>
            <goals>
              <goal>single</goal>
            </goals>
          </execution>
        </executions>
      </plugin>
    </plugins>
  </build>

```



#### Scala

```xml

    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
    </properties>
    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.12</artifactId>
            <version>3.0.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.12</artifactId>
            <version>3.0.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.12</artifactId>
            <version>3.0.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-yarn_2.12</artifactId>
            <version>3.0.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-10_2.12</artifactId>
            <version>3.0.0</version>
        </dependency>
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>druid</artifactId>
            <version>1.1.10</version>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-core</artifactId>
            <version>2.10.1</version>
        </dependency>
        <dependency>
            <groupId>com.esotericsoftware</groupId>
            <artifactId>kryo</artifactId>
            <version>5.0.3</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.12</artifactId>
            <version>3.0.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-exec</artifactId>
            <version>1.2.1</version>
        </dependency>
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.27</version>
        </dependency>
    </dependencies>

```



#### Flink

```xml

    <properties>
        <flink.version>1.13.0</flink.version>
        <java.version>1.8</java.version>
        <scala.binary.version>2.12</scala.binary.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-nop</artifactId>
            <version>1.7.2</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-java</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-java_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-clients_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-cep_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-java-bridge_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-scala_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-common</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-kafka_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-csv</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.47</version>
        </dependency>
        <dependency>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-assembly-plugin</artifactId>
            <version>3.3.0</version>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-assembly-plugin</artifactId>
                <version>3.3.0</version>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.8.1</version>
                <configuration>
                    <source>8</source>
                    <target>8</target>
                </configuration>
            </plugin>
        </plugins>
    </build>

```





## git

# Git

## 一、简介

### 1> Git是什么？

```bash
1. 是一个分布式版本控制系统
2. 以"行"为单位进行存储，可以监控每行的变化
3. 几乎所有的软件的代码管理现在都在使用git.
```

### 2> Git能做什么

```bash
   a、"版本还原"
   b、"代码备份"
   c、"分支管理"：秒级创建一个分支，分支进行代码编辑时，互不影响
   d、"冲突解决"：当不同分支对同一行代码进行变更时，前后提交，就会出现代码冲突，Git能快速高效解决冲突问题
   e、"历史追查"：可以看到代码修改的记录；
   f、"版本记录"：可以回到任何一个版本
   g、"权限管理"：针对不同的人开始不同的权限。

# commit很重要, 有了commit就相当于加了个保险
1.  在 Git 中任何已提交的东西几乎总是可以恢复的
2.  任何你未提交的东西丢失后很可能再也找不到了

   
```

### 3> Git配置的3个级别

```bash
1 系统默认，位于git安装路径下；
2 用户配置：在c盘下；名字叫 .gitconfig
3 项目配置：在当前项目仓库的配置文件中。
```

![image-20220219155353544](https://pictures-for-typora.oss-cn-beijing.aliyuncs.com/typora_imagetypora_imagetypora_imageimage-20220219155714825.png)



### 4>  Git的3个区转换

```bash
1.工作区(Working Directory):就是你电脑本地硬盘目录
2.本地库(Repository):工作区有个隐藏目录.git，它就是Git的本地版本库
3.暂存区(stage):一般存放在"git目录"下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）

# 上传过程：
   a、工作区 -> 暂存区 ：git add file
   b、暂存区 -> 本地仓库： git commit -m "备注信息"
# 工作区变更不想要：git restore file
   b、提交到暂存区，想打回给工作区： git restore --staged file 
   c、本地仓库的文件来覆盖工作区的文件： git checkout file
```



![image-20220219155714825](https://pictures-for-typora.oss-cn-beijing.aliyuncs.com/typora_imagetypora_imageimage-20220219155353544.png)

## 二、git的基本操作

##### 1. 设置Git账户

```bash
命令	含义
git config --list											查看所有配置
git config --list --show-origin				查看所有配置以及所在文件位置
git config --global user.name xxx			设置git用户名
git config --global user.email xxx		设置git邮箱
git init															初始化本地库
git config core.autocrlf false				取消换行符转换的warning提醒
```



##### 2. 常用命令

```bash
命令	作用
git status									查看本地库的状态(git status -s 简化输出结果)
git add [file]							多功能命令: 1. 开始跟踪新文件 
																		2. 把已跟踪的文件添加到暂存区 
																			3. 合并时把有冲突的文件标记为已解决状态
git commit –m “xxx” [file]	将暂存区的文件提交到本地库,-m 后面为修改的说明
```





#####  4. 忽略文件

- 一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。 
- 通常都是些自动生成的文 件，比如日志文件，或者编译过程中创建的临时文件等。 
- 在这种情况下，我们可以创建一个名为 .gitignore 的文件，列出要忽略的文件的模式。

```bash
--1. 使用场景
    a、当有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表种时。 
--2. 实现方式：
    a、创建".gitignore" 的文件，列出要忽略的文件的模式 --文件的名字不能错
    b、模式匹配规则
        # 忽略所有的.a 文件 
        *.a

        # 但跟踪所有的 lib.a，即便你在前面忽略了 .a 文件 
        !lib.a

        # 只忽略当前目录下的 TODO 文件，而不忽略 subdir/TODO. 不递归的忽略
        /TODO

        # 忽略任何目录下名为 build 的文件夹 递归的忽略
        build/

        # 忽略 doc/notes.txt，但不忽略 doc/server/arch.txt 
        doc/*.txt

        # 忽略 doc/ 目录及其所有子目录下的 .pdf 文件 
        doc/**/*.pdf
```



##### 5. 版本的切换

- git reset --hard HEAD^
- git reset --hard HEAD~n
- git reset --hard [具体版本号，例如：1f9a527等]

```bash
相关命令:

命令																	作用
git log															以完整格式查看本地库状态(查看提交历史)
git log --pretty=oneline						以单行模式查看本地库状态
git reset --hard HEAD^							回退一个版本
git reset --hard HEAD~n							回退N个版本
git reflog													查看所有操作的历史记录
git reset --hard [具体版本号]				回到（回退和前进都行）指定版本号的版本，
git checkout -- [file]						 从本地库检出最新文件覆盖工作区的文件(文件还没有提交到暂存区, 否则无效)
git reset [file]  或者 git restore –staged [file]	从暂存区撤销文件
git restore <file>									放弃在工作区的修改(还没有提交到暂存区)
git rm --cache [file]								撤销对文件的跟踪.
```



##### 6. 删除操作

- 文件夹内删除
- git add .
- git commit -m "delete"



##### 7. 比较文件

减号和加号分别代表两个对比的文件！

- $ git diff <file>
    - 将工作区中的文件和暂存区进行比较
- $ git diff HEAD <file>
    - 将工作区中的文件和本地库当前版本进行比较
- git diff --cached <file>
    - 查看暂存区和本地库最新提交版本的差别



##### 8. git目录

![image-20200622194001339](https://pictures-for-typora.oss-cn-beijing.aliyuncs.com/typora_imagetypora_imageimage-20200622194001339.png)

##### 9. 分支操作

```bash
命令	描述
git branch [分支名]							创建分支
git branch -v										查看分支,可以使用-v参数查看详细信息
git checkout [分支名]						切换分支
git merge [分支名]								合并分支；
																将merge命令中指定的分支合并到当前分支上
																例如：如果想将dev分支合并到master分支，那么必须在master分支上执行merge命令
																如果在dev分支上面，对一个文件做了修改，这个时候master分支上面对应的文件是没有修改信息的。
																因此需要将分支合并！
																
git branch –d[分支名]						删除分支
																注意：必须切换到master，才能删除，不能自杀！
																
git checkout –b [分支名]					新建并切换到当前分支
git log --oneline --decorate --graph --all	它会输出你的提交历史、各个分支的指向以及项目的分支分叉情况
```

概念:

- 不使用分支，就是人与人之间协作；
- 使用分支，就是小组与小组之间的协作；
- 从主干中拉取分支，开发完成，将工作，合并到主干。



##### 10. 分支冲突

问题: 当同时编辑一个文件时, 主分支和其他分支同时修改了同一行, 则合并时会引起冲突. 

解决: 编辑冲突的文件，把“>>>>>>>>>”、“<<<<<<”和“========”等这样的行删除，编辑至满意的状态，提交。

提交的时候注意：git commit命令不能带文件名。



## 三、github

![image-20220429163559365](https://pictures-for-typora.oss-cn-beijing.aliyuncs.com/typora_imagetypora_imageimage-20220429163559365.png)

##### 1. 本地连接Github

```bash
1. 创建一个密钥对
ssh-keygen
2. 复制公钥
catch ~/.ssh/id_rsa.pub
3. 粘贴到github中的ssh keys中
4. 测试 
ssh -T git@github.com
```



流程:

```
从本地和github通讯, 有两种方式:

1. 使用https
2. 使用ssh

推送:

1. 先建一个远程仓库
2. 给远程仓库起个别名
3. 推送

新员工入职:

1. 拿到所有代码

协作冲突:

当两个人修改了共一个行会发生冲突.

谁先push, 谁成功!

解决:

1. 后push要拉倒最新的版本()
2. 然后解决冲突
```



##### 2. push

```bash
本地库推送到GitHub
①准备本地库
②在GitHub上创建一个仓库
③增加远程地址
git remote add  <远端代号>   <远端地址> 
 								<远端代号> 是指远程链接的代号，一般直接用origin作代号，也可以自定义；
								<远端地址> 默认远程链接的url；
								
④本地库推送到远程库
git  push  -u  <远端代号>    <本地分支名称>
 							 <远端代号> 是指远程链接的代号；
 							 <分支名称>  是指要提交的分支名字，比如master；
 							 
我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。

⑤查看远程分支
		git remote –v
```



##### 3. pull

```
本地库抓取远程库
如果远程库的版本新于当前库，那么此时为了使当前库和远程库保持一致，可以执行pull命令
git pull <远端代号>  <远端分支名>;

例 git pull origin  master
```



##### 4. clone

```bash
实际的情况可能是刚开始做项目的时候，需要从远程库将项目先整到本机。
执行命令：git  clone   <远端地址>   <新项目目录名>
 										 <远端地址> 是指远程链接的地址;
										 <项目目录名>  是指为克隆的项目在本地新建的目录名称，可以不填，默认是GitHub的项目名;
命令执行完后，会自动为这个远端地址建一个名为origin的代号。

例 git  clone  https://github.com/user111/Helloworld.git   hello_world
```



##### 5. 解决冲突

尽量避免冲突:

1. 尽量不要修改同一文件
2. 如果多个人修改同一文件, 最好的办法, 找一个专人负责修改公共文件.
3. 每次写代码之前, 先更新到最新 版本. `上班第一件事, 先pull`
4. `上传值代码之前`, 也要先pull. 先把产生的冲突先解决, 再上传.



##### 6. 邀请成员

![image-20200622201523648](https://pictures-for-typora.oss-cn-beijing.aliyuncs.com/typora_imagetypora_imageimage-20200622202236769.png)



##### 7. 总结

![image-20200622202236769](https://pictures-for-typora.oss-cn-beijing.aliyuncs.com/typora_imagetypora_imageimage-20200622201523648.png)



## 四、github与idea

##### 1. 上传到github

1.  在idea中找到setting , 搜索git, 找到路径, 并关联github账号

2.  vcs ---> import into version control ---> share project on github
3.  勾掉.idea
4.  vcs ---> git ---> add ---> push

##### 

##### 2. clone 到idea

1. checkout from vresion control
2. 输入clone地址
3. 点更新/pull
